#!/usr/bin/env python3
"""
Task 1: Fine-Tuning LLMs using LoRA

Fine-tuning Large Language Models (LLMs) is a crucial technique for adapting pre-trained models 
to specific tasks or domains. Low-Rank Adaptation (LoRA) is an efficient method that significantly 
reduces the number of trainable parameters while maintaining performance. This approach is 
particularly useful for those with limited computational resources.

Author: LLM LoRA Project
Date: September 2025
"""

import sys
import os
from typing import Optional, Dict, Any

# Handle optional imports gracefully
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

# Task information for the menu system
TASK_INFO = {
    'title': 'Fine-Tuning LLMs using LoRA',
    'description': '''Fine-tuning Large Language Models (LLMs) is a crucial technique for adapting pre-trained models to specific tasks or domains. Low-Rank Adaptation (LoRA) is an efficient method that significantly reduces the number of trainable parameters while maintaining performance.

This task demonstrates:
‚Ä¢ Loading a pre-trained model (GPT-2)
‚Ä¢ Configuring LoRA parameters
‚Ä¢ Applying LoRA to the model
‚Ä¢ Displaying trainable parameters comparison

Key Benefits of LoRA:
‚Ä¢ Reduces memory usage significantly
‚Ä¢ Faster training times
‚Ä¢ Maintains model performance
‚Ä¢ Easy to switch between different adaptations''',
    'concepts': [
        'Low-Rank Adaptation (LoRA) fundamentals',
        'Parameter-efficient fine-tuning',
        'Model adaptation techniques',
        'Memory optimization strategies',
        'Trainable parameter reduction'
    ]
}


def check_dependencies():
    """Check if required packages are installed."""
    required_packages = ['transformers', 'peft', 'torch']
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        print(f"‚ùå Missing required packages: {', '.join(missing_packages)}")
        print("Please install them using:")
        print(f"pip install {' '.join(missing_packages)}")
        return False
    
    return True


def demonstrate_lora_setup():
    """Demonstrate basic LoRA setup with GPT-2."""
    print("üîß Setting up LoRA with GPT-2...")
    
    try:
        from transformers import AutoModelForCausalLM, AutoTokenizer
        from peft import get_peft_model, LoraConfig, TaskType
        
        # Model configuration
        model_name = "gpt2"
        print(f"üì¶ Loading model: {model_name}")
        
        # Load model and tokenizer
        model = AutoModelForCausalLM.from_pretrained(model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # Add padding token if it doesn't exist
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("‚úÖ Model and tokenizer loaded successfully")
        
        # Configure LoRA
        print("‚öôÔ∏è Configuring LoRA parameters...")
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=8,                    # Rank of adaptation
            lora_alpha=32,          # LoRA scaling parameter
            lora_dropout=0.1,       # Dropout probability
            bias="none",            # Bias type
        )
        
        # Get original parameter count
        original_params = sum(p.numel() for p in model.parameters())
        
        # Apply LoRA to the model
        print("üîÑ Applying LoRA to the model...")
        peft_model = get_peft_model(model, peft_config)
        
        # Get trainable parameter count
        trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in peft_model.parameters())
        
        # Display results
        print("\n" + "="*60)
        print("üìä PARAMETER COMPARISON")
        print("="*60)
        print(f"Original model parameters:     {original_params:,}")
        print(f"Total parameters with LoRA:   {total_params:,}")
        print(f"Trainable parameters:         {trainable_params:,}")
        print(f"Percentage trainable:         {100 * trainable_params / total_params:.2f}%")
        print(f"Parameter reduction:          {100 * (1 - trainable_params / original_params):.2f}%")
        print("="*60)
        
        # Display LoRA configuration
        print("\nüîß LoRA Configuration:")
        print(f"  ‚Ä¢ Rank (r): {peft_config.r}")
        print(f"  ‚Ä¢ Alpha: {peft_config.lora_alpha}")
        print(f"  ‚Ä¢ Dropout: {peft_config.lora_dropout}")
        print(f"  ‚Ä¢ Task Type: {peft_config.task_type}")
        print(f"  ‚Ä¢ Bias: {peft_config.bias}")
        
        # Show which modules are adapted
        print(f"\nüéØ Adapted modules:")
        for name, module in peft_model.named_modules():
            if hasattr(module, 'lora_A'):
                print(f"  ‚Ä¢ {name}")
        
        return {
            'model': peft_model,
            'tokenizer': tokenizer,
            'config': peft_config,
            'original_params': original_params,
            'trainable_params': trainable_params,
            'total_params': total_params
        }
        
    except ImportError as e:
        print(f"‚ùå Import error: {e}")
        print("Please ensure all required packages are installed.")
        return None
    except Exception as e:
        print(f"‚ùå Error during LoRA setup: {e}")
        return None


def demonstrate_model_info(result: Dict[str, Any]):
    """Display detailed model information."""
    if not result:
        return
    
    print("\n" + "="*60)
    print("üîç DETAILED MODEL INFORMATION")
    print("="*60)
    
    model = result['model']
    
    # Model architecture info
    print(f"Model type: {type(model).__name__}")
    print(f"Base model: {model.base_model.model.__class__.__name__}")
    
    # Memory usage estimation
    param_size = result['total_params'] * 4  # Assuming float32
    trainable_size = result['trainable_params'] * 4
    
    print(f"\nüíæ Memory Usage (estimated):")
    print(f"  ‚Ä¢ Total model size: {param_size / (1024**2):.1f} MB")
    print(f"  ‚Ä¢ Trainable parameters size: {trainable_size / (1024**2):.1f} MB")
    print(f"  ‚Ä¢ Memory savings: {(param_size - trainable_size) / (1024**2):.1f} MB")
    
    # LoRA specific information
    print(f"\nüéØ LoRA Adaptation Details:")
    lora_modules = []
    for name, module in model.named_modules():
        if hasattr(module, 'lora_A'):
            lora_modules.append(name)
            lora_A_params = module.lora_A.numel()
            lora_B_params = module.lora_B.numel()
            print(f"  ‚Ä¢ {name}:")
            print(f"    - LoRA A parameters: {lora_A_params:,}")
            print(f"    - LoRA B parameters: {lora_B_params:,}")
            print(f"    - Total LoRA params: {lora_A_params + lora_B_params:,}")
    
    print(f"\nüìà Efficiency Metrics:")
    efficiency = result['trainable_params'] / result['original_params']
    print(f"  ‚Ä¢ Parameter efficiency: {efficiency:.6f}")
    print(f"  ‚Ä¢ Compression ratio: {1/efficiency:.1f}x")


def main():
    """Main function to demonstrate LoRA fine-tuning setup."""
    print("üöÄ Task 1: Fine-Tuning LLMs using LoRA")
    print("="*60)
    
    # Check dependencies
    if not check_dependencies():
        return False
    
    print("‚úÖ All required packages are available")
    print()
    
    # Demonstrate LoRA setup
    result = demonstrate_lora_setup()
    
    if result:
        # Show detailed information
        demonstrate_model_info(result)
        
        print("\n" + "="*60)
        print("‚úÖ TASK 1 COMPLETED SUCCESSFULLY!")
        print("="*60)
        print("üéì Key Learning Points:")
        print("  ‚Ä¢ LoRA significantly reduces trainable parameters")
        print("  ‚Ä¢ Memory usage is greatly optimized")
        print("  ‚Ä¢ Model performance is maintained")
        print("  ‚Ä¢ Easy to apply to existing models")
        print("\nüí° Next Steps:")
        print("  ‚Ä¢ Try different LoRA configurations (r, alpha, dropout)")
        print("  ‚Ä¢ Experiment with different target modules")
        print("  ‚Ä¢ Apply to different model architectures")
        print("="*60)
        return True
    else:
        print("\n‚ùå Task 1 failed to complete")
        return False


def run_test():
    """Test function for the menu system."""
    print("üß™ Running Task 1 Test...")
    print("-" * 40)
    print("üìã TEST EXPLANATION:")
    print("This test verifies the core functionality of Task 1: Fine-Tuning LLMs using LoRA")
    print()
    print("üîç What this test checks:")
    print("  ‚Ä¢ Dependency availability (transformers, peft, torch)")
    print("  ‚Ä¢ LoRA configuration creation and validation")
    print("  ‚Ä¢ Module imports and function accessibility")
    print("  ‚Ä¢ Error handling for missing dependencies")
    print("  ‚Ä¢ Integration compatibility with the menu system")
    print()
    print("‚úÖ Expected outcome: All components should be properly configured")
    print("   and ready for LoRA fine-tuning when dependencies are installed.")
    print()
    print("üöÄ Starting test execution...")
    print("-" * 40)
    
    try:
        # Test dependency checking
        deps_ok = check_dependencies()
        if not deps_ok:
            print("‚ùå Dependency test failed")
            return False
        
        print("‚úÖ Dependencies test passed")
        
        # Test LoRA setup (simplified for testing)
        try:
            from transformers import AutoModelForCausalLM
            from peft import LoraConfig, TaskType
            
            # Just test configuration creation
            config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=8,
                lora_alpha=32,
                lora_dropout=0.1,
                bias="none",
            )
            
            print("‚úÖ LoRA configuration test passed")
            print("‚úÖ All tests passed!")
            return True
            
        except Exception as e:
            print(f"‚ùå LoRA test failed: {e}")
            return False
            
    except Exception as e:
        print(f"‚ùå Test failed with error: {e}")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

